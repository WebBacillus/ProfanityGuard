{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzIRxiV8aRG3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get update\n",
        "!apt-get install --reinstall build-essential --yes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# %pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "%pip install datasets==1.16.1\n",
        "%pip install transformers==4.28.0\n",
        "%pip install jiwer\n",
        "%pip install ipywidgets\n",
        "%pip install pythainlp==4.0.0\n",
        "%pip install --upgrade accelerate"
      ],
      "metadata": {
        "id": "fGZ6GQNaaUEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import (\n",
        "    load_dataset,\n",
        "    load_from_disk,\n",
        "    load_metric,)\n",
        "import s3fs\n",
        "from transformers import (\n",
        "    Wav2Vec2CTCTokenizer,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForCTC,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "import torchaudio\n",
        "import re\n",
        "import json\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "import IPython.display as ipd\n",
        "import ipywidgets as widgets"
      ],
      "metadata": {
        "id": "ubdHOWS9bgjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u2b16mLqbhyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset(\"/content/drive/MyDrive/data/th_common_voice_70.py\", \"th\")\n",
        "datasets"
      ],
      "metadata": {
        "id": "tPZwwBpBbi6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(example, tok_func = word_tokenize):\n",
        "    example['sentence'] = ' '.join(tok_func(example['sentence']))\n",
        "    return example\n",
        "\n",
        "datasets = datasets.map(preprocess_data)"
      ],
      "metadata": {
        "id": "ubzoCoGXbmD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"airesearch/wav2vec2-large-xlsr-53-th\")"
      ],
      "metadata": {
        "id": "OfsBRHmvbwVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1,\n",
        "                                             sampling_rate=16000,\n",
        "                                             padding_value=0.0,\n",
        "                                             do_normalize=True,\n",
        "                                             return_attention_mask=False)"
      ],
      "metadata": {
        "id": "UXC0Ruombxk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Ge0zh2MhcJ88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_file_to_array_fn(batch,\n",
        "                            text_col=\"sentence\",\n",
        "                            fname_col=\"path\",\n",
        "                            resampling_to=16000):\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[fname_col])\n",
        "    resampler=torchaudio.transforms.Resample(sampling_rate, resampling_to)\n",
        "    batch[\"speech\"] = resampler(speech_array)[0].numpy()\n",
        "    batch[\"sampling_rate\"] = resampling_to\n",
        "    batch[\"target_text\"] = batch[text_col]\n",
        "    return batch"
      ],
      "metadata": {
        "id": "hC6yaGkccKz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech_datasets = datasets.map(speech_file_to_array_fn,\n",
        "                                   remove_columns=datasets.column_names[\"train\"])\n",
        "speech_datasets"
      ],
      "metadata": {
        "id": "oDnjR1MScL1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # check that all files have the correct sampling rate\n",
        "    assert (\n",
        "        len(set(batch[\"sampling_rate\"])) == 1\n",
        "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
        "\n",
        "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
        "\n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "LHRCiXJwcMzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_datasets = speech_datasets.map(prepare_dataset,\n",
        "                                        remove_columns=speech_datasets.column_names[\"train\"],\n",
        "                                        batch_size=16,\n",
        "                                        batched=True)"
      ],
      "metadata": {
        "id": "BJDC75NgcN0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "U99qxH5HcO84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "metadata": {
        "id": "sLNKFq39cQBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wer_metric = load_metric(\"wer\")"
      ],
      "metadata": {
        "id": "mlkIzzg6cQ8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cer_metric = load_metric('cer')"
      ],
      "metadata": {
        "id": "87nK1OxMcR1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred, processor, metric):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "en63mFsvcS93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "qpLEpx6JcUQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"airesearch/wav2vec2-large-xlsr-53-th\",\n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    gradient_checkpointing=True,\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")"
      ],
      "metadata": {
        "id": "-WjwdwxDcVf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.freeze_feature_extractor()"
      ],
      "metadata": {
        "id": "HJOh1WXjcW6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/wav2vec2-detect-toxic-th\",\n",
        "    group_by_length=True,\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=1,\n",
        "    per_device_eval_batch_size=16,\n",
        "    metric_for_best_model='wer',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    num_train_epochs=40,\n",
        "    push_to_hub = True,\n",
        "    #fp16=True,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=30,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ],
      "metadata": {
        "id": "B5-O8g0JcXv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=partial(compute_metrics, metric=wer_metric, processor=processor),\n",
        "    train_dataset=prepared_datasets[\"train\"],\n",
        "    eval_dataset=prepared_datasets[\"validation\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "m7uOfcE6cZI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "hMfmJ1m1cafq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "8ILwP8l5ccO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(batch):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs.input_values.to(device),).logits\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    batch[\"pred_sentence\"] = processor.batch_decode(pred_ids)\n",
        "    return batch\n",
        "\n",
        "wer_metric = load_metric(\"wer\")\n",
        "cer_metric = load_metric(\"cer\")"
      ],
      "metadata": {
        "id": "ctX0aFTzcd7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = load_dataset(\"/content/drive/MyDrive/data/th_common_voice_70.py\", \"th\", split=\"test\")\n",
        "test_dataset"
      ],
      "metadata": {
        "id": "pRXh38gtcfWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_file_to_array_fn(batch,\n",
        "                            text_col=\"sentence\",\n",
        "                            fname_col=\"path\",\n",
        "                            resampling_to=16000):\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[fname_col])\n",
        "    resampler=torchaudio.transforms.Resample(sampling_rate, resampling_to)\n",
        "    batch[\"speech\"] = resampler(speech_array)[0].numpy()\n",
        "    batch[\"sampling_rate\"] = resampling_to\n",
        "    batch[\"target_text\"] = batch[text_col]\n",
        "    return batch\n",
        "\n",
        "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
        "inputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)"
      ],
      "metadata": {
        "id": "B8zmUlgHcge5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs.input_values).logits\n",
        "\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "print(\"Prediction:\", processor.batch_decode(predicted_ids))\n",
        "print(\"Reference:\", test_dataset[\"sentence\"][:2])"
      ],
      "metadata": {
        "id": "TYUR8nk-cnc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "result = test_dataset.map(evaluate, batched=True, batch_size=8)\n",
        "result_df = pd.DataFrame({'sentence':result['sentence'],\n",
        "                           'pred_sentence_tok': result['pred_sentence']})"
      ],
      "metadata": {
        "id": "T_XWqIencou5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in result_df.iterrows():\n",
        "  result_df.loc[index, 'sentence'] = row['sentence'].replace(\" \", \"\")\n",
        "  result_df.loc[index, 'pred_sentence_tok'] = row['pred_sentence_tok'].replace(\" \", \"\")"
      ],
      "metadata": {
        "id": "-cLd5_MOcpyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df['sentence_tok'] = result_df.sentence.map(lambda x: ' '.join(word_tokenize(x)))\n",
        "result_df['pred_sentence'] = result_df.pred_sentence_tok.map(lambda x: ''.join(x.split()))\n",
        "#change tokenization to fit pythainlp tokenization\n",
        "result_df['pred_sentence_tok'] = result_df.pred_sentence.map(lambda x: ' '.join(word_tokenize(x)))"
      ],
      "metadata": {
        "id": "Xgh-ayiJcrX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  result_df.to_csv('result.csv')"
      ],
      "metadata": {
        "id": "wopvxO0yctbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "result_df = pd.read_csv('/content/result.csv')\n",
        "result_df"
      ],
      "metadata": {
        "id": "h3PkPZOacxrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wer_metric.compute(predictions=result_df.pred_sentence_tok,references=result_df.sentence_tok)"
      ],
      "metadata": {
        "id": "nK5DGv-Hc0hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cer_metric.compute(predictions=result_df.pred_sentence,references=result_df.sentence)"
      ],
      "metadata": {
        "id": "1PbP1_vgc1PA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}